{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioindicators of Strawberry Creek\n",
    "### Professors Mary Power, John Huelsenbeck & Bruce Baldwin\n",
    "_Estimated Time: 50 minutes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this lab you will be using data science tools to determine the significance between the ecological health of the two branches of Strawberry Creek.\n",
    "\n",
    "**Learning Outcomes**\n",
    "\n",
    "By the end of the notebook, students should be able to:\n",
    "\n",
    "1. Explain the use of biological organisms as indicators of ecosystem health\n",
    "2. Define dichotomous keys and use them to ID organisms\n",
    "3. Interpret biological metrics: taxa richness, %EPT, Biotic Index, % Filterers, % Predators\n",
    "4. Analyze simulated resampling to determine if two assemblages of organisms are different fro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. [Jupyter Notebooks](#1)\n",
    "    - [Types of Cells](#1.1)\n",
    "    - [Running Cells](#1.2)\n",
    "    - [Editting, Saving and Submitting](#1.3)\n",
    "    - [Debugging Tips and Jupyter Help](#1.4)\n",
    "<br/><br/>\n",
    "2. [Introduction to Data Analytics](#2)\n",
    "    - [Null and Alternate Hypothesis](#2.1)\n",
    "    - [Permutation Test](#2.2)\n",
    "    - [Bootstrapping](#2.3)\n",
    "<br/><br/>\n",
    "3. [Introduction to Data Analytics](#3)\n",
    "    - [Experiment 1](#3.1)\n",
    "    - [Experiment 2](#3.2)\n",
    "    - [Experiment 3](#3.3)\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is currently set up in a Jupyter Notebook. A Jupyter Notebook is an online, interactive computing environment, composed of different types of __cells__. Cells are chunks of code or text that are used to break up a larger notebook into smaller, more manageable parts and to let the viewer modify and interact with the elements of the notebook.\n",
    " \n",
    "### Types of cells <a id= '1.1'> </a>\n",
    "\n",
    "There are two types of cells in Jupyter, __code__ cells and __markdown__ cells. Code cells are cells indicated with “In [ ]:” to the left of the cell. In these cells you can write you own code and run the code in the individual cell.\n",
    "Markdown cells hold text a majority of the time and do not have the “In [ ]” to the left of the cell (Just an empty space). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running cells <a id= '1.2'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Running\" a cell is similar to pressing 'Enter' on a calculator once you've typed in an expression; it computes all of the expressions contained within the cell.\n",
    "\n",
    "To run a code cell, you can do one of the following:\n",
    "- press __Shift + Enter__\n",
    "- click __Cell -> Run Cells__ in the toolbar at the top of the screen.\n",
    "\n",
    "You can navigate the cells by either clicking on them or by using your up and down arrow keys. Try running the cell below to see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the cell consists of the text/code that is contained within the cell's enclosing box. Here, the input is an expression in Python that \"prints\" or repeats whatever text or number is passed in. \n",
    "\n",
    "The output of running a cell is shown in the line immediately after it. Notice that markdown cells have no output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing, Saving and Sumbitting <a id='1.3'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To __edit__ a cell simply click on the desired cell and begin typing \n",
    "- To __save__ your notebook press command + s on the keyboard \n",
    "- We will go into the specifics of how to __submit__ your work at the end of the lab, but you will essentially be converting your work into a PDF file and then submitting it to bCourses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Tips and Jupyter Help <a id= '1.4'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='2'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the course of this lab you will be using Python to analyze the data that you collected from Strawberry Creek. Python allows us to use data analysis methods that simulate data sets that we may not have the resources to collect in real life. The main purpose of this lab is to determine whether or not the ecological health of the two branches of the creek have a significant different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qgrid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f0a61f4ef26b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mqgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qgrid'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qgrid\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Analytics <a id= '2'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis vs. Alternate Hypothesis <a id='2.1'> </a>\n",
    "\n",
    "One of the first problems to work through when looking at a data set is to determine whether or not the trends in the data are significant or purely due to random chance. In this particular lab we are trying to determine whether or not the difference between the ecological healths of the two branches of the creek are significant or if it is due to chance. To do this we begin by forming a null hypothesis and an alternative hypothesis to test. \n",
    "\n",
    "__Null Hypothesis__: A null hypothesis claims that there is no statistical difference between two distributions and that any difference is due to experimental error or chance.\n",
    "\n",
    "__Alternative Hypothesis__: An alternative hypothesis essentially counters the null hypothesis and claims that the difference in distribution is significant.\n",
    "\n",
    "Example Null and Alternative Hypothesis\n",
    "\n",
    "Say we have a data set with data on the number of boba shops on Southside and Northside. The data set shows that Southside has a higher average of boba shops than Northside, but it is unclear whether the difference in the average is due to chance or some other unknown reason. For this data set potential hypotheses would be:\n",
    "\n",
    "__Example Null Hypothesis__\n",
    "- The distribution of the average of boba shops is the same for the samples taken from Southside as the samples taken from Northside. The difference in sample distribution is due to chance. \n",
    "\n",
    "__Example Alternative Hypothesis__  \n",
    "- The average of boba shops in Northside is lower than the average of boba shops in Northside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would be a potential null hypothesis for this lab?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would be a potential alternative hypothesis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have your null and alternative hypothesis, the next step is to simulate the distribution under the null hypothesis! Theoretically, if the difference in distribution were solely due to random chance, then the data that the distribution originally comes from would not matter. This is where permutation tests come in to play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test <a id='2.2'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A permutation test essentially shuffles the given data set and creates new distributions. In this case, we are using a permutation test to shuffle the difference in ecological health of the two creeks. As was previously mentioned, permutation tests simulate the null hypothesis because it assumes that there is no significant difference between the distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we will run permutation testing on example data of FBI scores collected from the North and South Fork. Run the following code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# north_fork_example = pd.DataFrame({\n",
    "#     'FBI Score':[3.5, 4.0, 3.0, 3.5, 4.2],\n",
    "#     'Mean FBI Score':[3.64, 3.64, 3.64, 3.64, 3.64],\n",
    "#     'Score - Mean':[-0.14, 0.36, -0.64, -0.14, 0.56],\n",
    "#     'Square of Difference':[0.0196, 0.1296, 0.4096, 0.0196, 0.3136]\n",
    "# })\n",
    "\n",
    "# north_fork_example\n",
    "\n",
    "example = pd.DataFrame({\n",
    "    'FBI Score':[3.5, 4.0, 3.0, 3.5, 4.2, 4.5, 5.0, 3.6, 4.9, 5.1, 3.4, 2.9],\n",
    "    'Fork':np.append(np.repeat('North', 5), np.repeat('South', 7))\n",
    "})\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example data, we have 5 data points from the North Fork and 7 from the South Fork. Run the cell below to see the observed difference in FBI Score means between the two forks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_difference = example[example['Fork']=='North'].mean() - example[example['Fork']=='South'].mean()\n",
    "observed_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this our observed difference because this statistic is observed from data that is actually collected (although generated in our case).\n",
    "\n",
    "In permutation testing, we will be shuffling the data points between the two forks. For one permutation, we will calculate the FBI Score means for each fork. In this case, the mean difference is no longer an observed difference but a simulated difference. Run the cell below to generate a permutation of the data and to calculate the new difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_example = pd.DataFrame({\n",
    "    'FBI Score':example['FBI Score'].sample(len(example['FBI Score'])),\n",
    "    'Fork':np.append(np.repeat('North', 5), np.repeat('South', 7))\n",
    "})\n",
    "perm_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_difference = perm_example[perm_example['Fork']=='North'].mean() - perm_example[perm_example['Fork']=='South'].mean()\n",
    "perm_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just for one permutation of the data. Now we perform the permutation test many more times, and with these values we can plot the distribution of differences. Using this distribution of simulated differences, we can compare it with our actual observed difference to see how likely it is to observe this difference and if our null hypotheis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_in_means(fbi_scores):\n",
    "    return np.mean(fbi_scores[:5]) - np.mean(fbi_scores[5:])\n",
    "\n",
    "n_repeats = 1000\n",
    "permutation_differences = []\n",
    "for i in range(n_repeats):\n",
    "    permutation = example['FBI Score'].sample(len(example['FBI Score']))\n",
    "    new_difference = difference_in_means(permutation)\n",
    "    permutation_differences.append(new_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(permutation_differences)\n",
    "plt.axvline(observed_difference[0], color='red', label='Observed Difference')\n",
    "plt.xlabel('FBI Score Mean Difference')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, we can guess if the null hypothesis is true (the observed difference between the two branches is due to random chance) or if the alternative hypothesis is true (that it is not due to chance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How likely is it for the observed difference to occur, and can we reject the null hypothesis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Based on the distribution, although it's not very likely to occur, we still can't rule out the possibility of getting the observed difference by random chance, because it is not far enough in the tail of the distribution to be considered as very unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping <a id='2.3'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem that often surfaces when analysing a data set is the accuracy of an estimated statistic. For example, if we wanted to provide an estimate of the average difference in ecological health between North and South Fork, the calculations from just the experiment would not be representative of the difference throughout the day, or even throughout the year. On the otherhand, it would also not be feasible to go around collecting samples and calculating the ecological health every single hour. This is where bootstrapping comes into play!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping generates new random samples by drawing samples from the original data set. We essentially treat our data set as the population. We randomly draw from the data set __with replacement__ to create new data sets that are the same size as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_example = perm_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_difference = boot_example[boot_example['Fork']=='North'].mean() - boot_example[boot_example['Fork']=='South'].mean()\n",
    "obs_difference.at[\"FBI Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the data collected from your individual experiment, the estimated difference in Family Biotic Index (FBI) is the number displayed after running the cell above. As we previously described, we cannot be sure that this is a good estimate of the difference in FBI Score from this analysis alone. One solution is to use bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = example.sample(n = 12, replace = True)\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the bootstrapping method once, we have a new average difference in FBI Score displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_sample[new_sample['Fork']=='North'].mean().at[\"FBI Score\"] - new_sample[new_sample['Fork']=='South'].mean().at[\"FBI Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the bootstrapping method many times and compile the calculated average FBI differences into one distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FBI_averages = []\n",
    "for i in np.arange(500):\n",
    "    one_new_sample = example.sample(n = 12, replace = True)\n",
    "    average = one_new_sample[one_new_sample['Fork']=='North'].mean() - one_new_sample[one_new_sample['Fork']=='South'].mean()\n",
    "    FBI_averages.append(average)\n",
    "avgs_tbl = pd.DataFrame(FBI_averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resampling and calculating the average difference in FBI Scores 100 times, we get this graph that displays the distribution of the average differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_tbl.hist(\"FBI Score\")\n",
    "plt.hist(obs_difference, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically the data set that is collected represents the population, so the distribution of the original sample will resemble the distribution of the population. Similarly, the resampled data sets will resemble the original data set and therefore the population, which is why resampling from the same sample works for bootstrapping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__P-Values & Statistical Significance__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a distribution of what the differences in FBI Scores will generally look like, the next step is to determine how likely it is under the null hypothesis for the difference to be equal to or even higher than the one observed in our experiment sample. This likelihood is more classically known as the P-value of a test. The P-value of a test is the chance that, under the null hypothesis, the test statistic will be equal to the observed statistic or lean more towards the direction that supports the alternative hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculating P-values__\n",
    "\n",
    "For the purpose of this experiment, the p-value will represent the chance that the difference in FBI Scores is greater than or equal to the observed difference. \n",
    "To calculate the p-value we would count the number of times the difference is above or equal to the observed difference in the bootstrapped distribution and divide it by the total amount of bootstrap repetitions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_val_count = 0\n",
    "for i in np.arange(500):\n",
    "    in_p_val = avgs_tbl.at[i, \"FBI Score\"] >= obs_difference.at[\"FBI Score\"]\n",
    "    if in_p_val == True:\n",
    "        p_val_count += 1\n",
    "p_val_count / 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the P-value is small, then that implies that it is very unlikely for this statistic to occur under the null hypothesis and we say we “reject the null hypothesis”. Otherwise, if the P-value is large, then that implies that the observed test statistic has a high likelihood of occurring under the null and we say we “fail to reject the null hypothesis”. \n",
    "\n",
    "A conventional cut-off for P-values is 5%. If the P-value is less than or equal to 5%, then the p-value is deemed “statistically significant”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the calculated P-value above, do we reject the null hypothesis or fail to reject the null hypothesis? Why?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_type answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notebook developed by: Joshua Asuncion, Karalyn Chong\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
